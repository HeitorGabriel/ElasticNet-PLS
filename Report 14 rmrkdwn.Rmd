---
title: "Elastic Net e Mínimos Quadrados Parciais" 
author: "Heitor Gabriel S. Monteiro"
date: "22/11/2021"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 12pt
output:
  html_document:
    highlight: tango
    theme: cerulean
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: true
    number_sections: true
    highlight: tango
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prelúdio

Nosso objetivo é exercitar UM algoritmo de redução de dimensão (**Mínimos Quadrados Parciais**) e outro de regularização e encolhimento de estimadores da regressão linear: o (**Elastic Net**), que faz uma combinação linear entre a penalização Lasso ($\alpha =1$) e Ridge ($\alpha =0$) com o parâmetro `mixture`, na estrutura do [Tidymodels](https://www.tidymodels.org/). E $\lambda$ sendo a importância da regularização, ou o grau de penalização, que é o parâmetro `penalty`. A estrutura do Elastic Net é:

$$\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)\|\beta\|_2^2/2 + \alpha \|\beta\|_1\right],$$

Para importar [os dados](https://rdrr.io/cran/ISLR/man/Hitters.html), vamos já retirar os `NA` usando `na.omit()`.

```{r, warning=FALSE, message=FALSE}
setwd('/home/heitor/Área de Trabalho/R Projects/Análise Macro/Labs/Lab 14')

library(tidyverse)   # padrão manipulação e visualização
library(tidymodels)  # padrão para ML
library(glmnet)      # Elastic Net
library(plsmod)      # Partial Least Squares
library(plotly)      # Gráficos interativos
library(GGally)      # Gráfico de Correlação
library(ISLR)        # Base de Dados Hitters
library(rmarkdown)   # Tabelas paginadas

set.seed(2022)       # Aleatoriedade fixa em número auspicioso

dds <- as.data.frame(Hitters) %>%
	as_tibble() %>% na.omit()
```

# Estatísticas Descritivas 

Reparemos que há muitas variáveis altamente correlacionadas e outras sem correlação. Para nossos fins de regredir `Salary`, realmente precisamos de um algoritmo de seleção de variáveis, ou redução delas.

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.height=4, fig.width=9}
dds %>% summary()
ggcorr(dds %>%
	   	select(!c('NewLeague', 'League', 'Division')),
	   label = T)
```

# Divisão: Treino & Teste

Vamos fazer a primeira divisão entre treino e teste. Aplicaremos a reamostragem somente nos dados de treino, conforme [Kuhn & Johnson (2019)](https://bookdown.org/max/FES/resampling.html).

```{r}
slice_1   <- initial_split(dds)
train_dds <- training(slice_1)
test_dds  <- testing(slice_1)
```

# Modelo

Formaremos agora dois modelos: o primeiro Elastic Net conforme a equação acima; o segundo será usando Mínimos Quadrados Parciais, com o número fixo de sete novos regressores, criados a partir da combinação linear dos regressores dos originais. Para detalhes, veja [Lavine e Rayens (2019)](https://www.sciencedirect.com/science/article/pii/B9780444527011000247). Repare que os parâmetros de afinação `penalty` e `mixture` estão para ser testados no *cross validation*, para escolhermos o melhor.

```{r}
algorit_elast <- linear_reg(
	penalty = tune::tune(),
	mixture = tune::tune()) %>%
	set_mode('regression') %>% 
	set_engine("glmnet")
algorit_elast

algorit_pls <- pls(num_comp = 7) %>% 
	set_mode("regression") %>% 
	set_engine("mixOmics")
algorit_pls
```

# Fórmula

Para preparar os dados para ser aplicado no modelo, aplicaremos o `step_normalize(all_numeric_predictors())` para normalizar todas as variáveis numéricas e `step_dummy(all_nominal_predictors())` para tranformar variáveis fator em *dummies*. A função `bake()` "cozinha" e nos mostra como ficarão os dados.

```{r}
formula_geral <- recipe(Salary~.,
						data = train_dds) %>% 
	step_normalize(all_numeric_predictors()) %>% 
	step_dummy(all_nominal_predictors()) %>%
	prep()

formula_geral %>%
	bake(new_data=NULL) %>% 
	rmarkdown::paged_table(options = list(
		rows.print = 10, 
		cols.print = 10))
```

# Workflow

Definiremos dois procedimentos por termos dois modelos diferentes. Nele, vamos alimentar a estrutura dos modelos com a fórmula transformada. Repare que, como não há nada para ser *tunado* na nossa regressão usando *pls*, já aplico a divisão final `last_fit(slice_1)`. Retomaremos a `wrkflw_pls` depois que afinarmos o Elastic Net.

```{r}
wrkflw_elast <- workflow() %>%
	add_model(algorit_elast) %>%
	add_recipe(formula_geral)

wrkflw_pls <- workflow() %>%
	add_model(algorit_pls) %>%
	add_recipe(formula_geral)

wrkflw_pls <- wrkflw_pls %>% 
	last_fit(slice_1)
```

# Validação

Definiremos o método de reamostragem para validação cruzada:

```{r}
kfold_geral <- vfold_cv(train_dds,
					 v=10,
					 repeats = 2)
```

# Ajustes e Treinamentos

## Limites do `tune()`

Montaremos pares de parâmetros a serem afinados. Veja que `mixture` vai de 0 (Ridge) a 1 (Lasso):

```{r}
grid_padr <- wrkflw_elast %>%
	parameters() %>% 
	update(
		penalty = penalty(range = c(.25, .75)),
		mixture = mixture(range = c(0, 1))
	) %>% 
	grid_regular(levels = 5)
grid_padr %>% 
	rmarkdown::paged_table(options = list(
		rows.print = 10, 
		cols.print = 2))
```

## Afinação dos `tune()`

Apesar de montar dois parâmetros de métrica, usaremos o `rmse`, que é mais sensível por pesar mais outliers.

```{r}
afinç_cv_elast <- wrkflw_elast %>% 
	tune_grid(resamples = kfold_geral,
			  grid      = grid_padr,
			  control   = control_grid(save_pred = T),
			  metrics   = metric_set(rmse, mae))

wrkflw_pls$.metrics
```

# Seleção do melhor modelo

Reparemos que interessante: o Ridge (`mixture = 0`) sai de melhor e se torna o pior à medida que o peso dos preditores adicionais cresce.

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.height=4, fig.width=7}
afinç_cv_elast %>% ggplot2::autoplot()
```

Ainda sim, pelo `show_best(n=1, 'rmse')`, vemos que algoritmo dos mínimos quadrados parciais performa melhor que a da regularização e penalização:

```{r}
afinç_cv_elast %>% show_best(n=1, 'rmse')
wrkflw_pls %>% show_best(n=1, 'rmse')

melhor_tune <- select_best(afinç_cv_elast, 'rmse') 
```

## Finalmentes

Agora, extrairemos o melhor par de parâmetros do Elastic Net, montaremos o procedimento do `workflow()` e aplicaremos na amostra de teste. Lembremos que já fazemos isso para o *pls*, já que não havia nada a ser afinado.

```{r}
algorit_fnl_elast <- algorit_elast %>% 
	finalize_model(parameters = melhor_tune)

wrkflw_fnl_elast <- workflow() %>% 
	add_model(algorit_fnl_elast) %>% 
	add_recipe(formula_geral) %>% 
	last_fit(slice_1)

wrkflw_fnl_elast$.predictions 
```

Agora, para representação, faremos os dois plots das duas técnicas, comparando estimados e reais:

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.height=4, fig.width=9, fig.cap='Modelo Elastic Net'}
ggplotly(
wrkflw_fnl_elast %>%
	collect_predictions() %>%
	ggplot(aes(x=.pred, y=Salary)) +
	geom_point() +
	geom_abline(intercept = 0,
				slope     = 1,
				color     = 'darkviolet',
				size      =.8))
```


```{r, warning=FALSE, message=FALSE, fig.align='center', fig.height=4, fig.width=9, fig.cap='Modelo de Mínimos Quadrados Parciais'}
ggplotly(
	wrkflw_pls %>%
		collect_predictions() %>% 
		ggplot(aes(x=.pred, y=Salary)) +
		geom_point() +
		geom_abline(intercept = 0,
					slope     = 1,
					color     = 'yellow3',
					size      =.8))
```

